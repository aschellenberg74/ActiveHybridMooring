{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mooring Load/Deflection Analysis for VolturnUS 15MW Floating Wind Turbine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3])\n",
    "np.repeat(x, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get MoorDyn output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = 'F:/Documents/WETO Incubator/OpenFAST/VolturnUS15MW' #change to directory containing MoorDyn output files\n",
    "md_out_files = glob.glob(os.path.join(datadir, '*_500m.MD.Line*.out'))\n",
    "numlines = len(md_out_files)\n",
    "numnodes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dfs = []\n",
    "for out_file in md_out_files:\n",
    "    linenum = out_file[-5] # only valid for single-digit lines with filenames ending with '.out'\n",
    "    out_df = pd.read_csv(out_file, sep='\\s+', skiprows=[1], na_values=['***************'])\n",
    "    out_df.columns = out_df.columns.str.replace('^Node', f'L{linenum}Node', regex=True)\n",
    "    out_df.columns = out_df.columns.str.replace('^Seg', f'L{linenum}Seg', regex=True)\n",
    "    out_dfs.append(out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdout = pd.concat(out_dfs, axis=1)\n",
    "mdout = mdout.loc[:, ~mdout.columns.duplicated()]\n",
    "mdout = mdout.loc[mdout['Time']>90, :].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_scaling(basin_depth, basin_length, basin_width, full_depth, full_fp_length, full_fp_width):\n",
    "    depth_scaling = full_depth / basin_depth\n",
    "    fp_length_scaling = full_fp_length / basin_length\n",
    "    fp_width_scaling = full_fp_width / basin_width\n",
    "    return np.max([depth_scaling, fp_length_scaling, fp_width_scaling])\n",
    "\n",
    "def scale_length(full_scale, scale_factor):\n",
    "    return full_scale / scale_factor\n",
    "\n",
    "def scale_force(full_scale, scale_factor):\n",
    "    return full_scale / scale_factor ** 3\n",
    "\n",
    "def scale_freq(full_scale, scale_factor):\n",
    "    return full_scale / scale_factor ** -0.5\n",
    "\n",
    "def get_power_spectrum(ts, dt):\n",
    "    sp2 = np.fft.fft(ts, axis=0)\n",
    "    sp = np.abs(sp2)[:len(sp2) // 2] * 2 # single-sided spectrum\n",
    "    sp_rms_avg = np.sqrt(np.mean(sp**2, axis=1))[1:]\n",
    "    freq = np.fft.fftfreq(len(ts), d=dt)[1:len(sp)]\n",
    "    return sp_rms_avg, freq\n",
    "\n",
    "def reshape_data(data, num_sets):\n",
    "    return np.reshape(data, (-1, num_sets))\n",
    "\n",
    "def get_dom_freqs(sp, freqs, tol):\n",
    "    sp_norm = sp / np.sum(sp)\n",
    "    return freqs[np.flatnonzero(sp_norm > tol)]\n",
    "\n",
    "def freq_assessment(df, var, tol, num_sets):\n",
    "    dt = df.loc[1, 'Time'] - df.loc[0, 'Time']\n",
    "    datasets = reshape_data(df[var], num_sets)\n",
    "    sp, freq = get_power_spectrum(datasets, dt)\n",
    "    return get_dom_freqs(sp, freq, tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get maximum loads/deflections per mooring line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs = {\n",
    "    'Depth': np.zeros(numnodes+1),\n",
    "    'Footprint Length': np.zeros(numnodes+1),\n",
    "    'Footprint Width': np.zeros(numnodes+1),\n",
    "}\n",
    "for l in range(numlines):\n",
    "   maxs[f'L{l+1} Max Hor Disp'] = np.zeros(numnodes+1)\n",
    "   maxs[f'L{l+1} Max Vert Disp'] = np.zeros(numnodes+1)\n",
    "   maxs[f'L{l+1} Max Disp Mag'] = np.zeros(numnodes+1)\n",
    "   maxs[f'L{l+1} Max Tension'] = np.zeros(numnodes+1)\n",
    "   maxs[f'L{l+1} Peak Freqs'] = []\n",
    "\n",
    "for n in range(numnodes+1):\n",
    "    max_depth = np.abs(mdout.loc[0, f'L1Node{n}pz'])\n",
    "    fp_length = np.abs(mdout.loc[n, f'L1Node{n}px'] - mdout.loc[n, f'L2Node{n}px'])\n",
    "    fp_width = np.abs(mdout.loc[n, f'L2Node{n}py'] - mdout.loc[n, f'L3Node{n}py'])\n",
    "    for l in range(numlines):\n",
    "      if n == 0:\n",
    "        max_ten = np.nan\n",
    "        peak_freqs = np.nan\n",
    "      else:\n",
    "        peak_freqs = freq_assessment(mdout, f'L{l+1}Seg{n}Ten', 0.05, 20)\n",
    "        max_ten = mdout[f'L{l+1}Seg{n}Ten'].max()\n",
    "      hor_disp = np.sqrt(\n",
    "          (mdout[f'L{l+1}Node{n}px'] - mdout.loc[0, f'L{l+1}Node{n}px'])**2\n",
    "        + (mdout[f'L{l+1}Node{n}py'] - mdout.loc[0, f'L{l+1}Node{n}py'])**2\n",
    "      )\n",
    "      vert_disp = mdout[f'L{l+1}Node{n}pz'] - mdout[f'L{l+1}Node{n}pz'][0]\n",
    "      mag_disp = np.sqrt(\n",
    "          (mdout[f'L{l+1}Node{n}px'] - mdout.loc[0, f'L{l+1}Node{n}px'])**2\n",
    "        + (mdout[f'L{l+1}Node{n}py'] - mdout.loc[0, f'L{l+1}Node{n}py'])**2\n",
    "        + (mdout[f'L{l+1}Node{n}pz'] - mdout.loc[0, f'L{l+1}Node{n}pz'])**2\n",
    "      )\n",
    "      max_hor_disp = hor_disp.max()\n",
    "      max_vert_disp = vert_disp.max()\n",
    "      max_mag_disp = mag_disp.max()\n",
    "\n",
    "      # maxs['Partition Node'][n] = int(n)\n",
    "      maxs[f'L{l+1} Max Disp Mag'][n] = max_mag_disp\n",
    "      maxs[f'L{l+1} Max Hor Disp'][n] = max_hor_disp\n",
    "      maxs[f'L{l+1} Max Vert Disp'][n] = max_vert_disp\n",
    "      maxs[f'L{l+1} Max Tension'][n] = max_ten\n",
    "      maxs[f'L{l+1} Peak Freqs'].append(peak_freqs)\n",
    "    maxs['Depth'][n] = max_depth\n",
    "    maxs['Footprint Length'][n] = fp_length\n",
    "    maxs['Footprint Width'][n] = fp_width\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as `pandas` `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_maxs = pd.DataFrame(data=maxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_maxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check time series tension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mdout['Time'], mdout['L1Seg50Ten'], label='Line 1')\n",
    "plt.plot(mdout['Time'], mdout['L2Seg50Ten'], label='Line 2')\n",
    "plt.plot(mdout['Time'], mdout['L3Seg50Ten'], label='Line 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess overall maxes at each node (aka partitioning point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMaine basin bounds (to determine possible scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMaine basin dimensions (in meters)\n",
    "w2_length = 30\n",
    "w2_width = 9\n",
    "w2_depth = 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take highest value from all mooring lines of previous `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_maxs = maxs = {\n",
    "    'Max Hor Disp': np.zeros(numnodes),\n",
    "    'Max Vert Disp': np.zeros(numnodes),\n",
    "    'Max Disp Mag': np.zeros(numnodes),\n",
    "    'Max Tension': np.zeros(numnodes),\n",
    "    'Peak Freqs': [],\n",
    "    'Physical Depth': np.zeros(numnodes),\n",
    "    'Footprint Length': np.zeros(numnodes),\n",
    "    'Footprint Width': np.zeros(numnodes),\n",
    "    'Max Scaling': np.zeros(numnodes),\n",
    "    'Scaled Hor Disp': np.zeros(numnodes),\n",
    "    'Scaled Vert Disp': np.zeros(numnodes),\n",
    "    'Scaled Disp Mag': np.zeros(numnodes),\n",
    "    'Scaled Tension': np.zeros(numnodes),\n",
    "    'Scaled Peak Freqs': []\n",
    "}\n",
    "partition_nodes = np.zeros(numnodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1, numnodes+1):\n",
    "    max_phys_depth = node_maxs.loc[numnodes-n:, 'Depth'].max()\n",
    "    max_fp_length = node_maxs.loc[numnodes-n:, 'Footprint Length'].max()\n",
    "    max_fp_width = node_maxs.loc[numnodes-n:, 'Footprint Width'].max()\n",
    "    max_hor_disp = node_maxs.filter(regex='Max Hor Disp$').loc[n, :].max()\n",
    "    max_vert_disp = node_maxs.filter(regex='Max Vert Disp$').loc[n, :].max()\n",
    "    max_mag_disp = node_maxs.filter(regex='Max Disp Mag$').loc[n, :].max()\n",
    "    max_seg_ten = node_maxs.filter(regex='Max Tension$').loc[n, :].max()\n",
    "    peak_freqs = np.unique(np.concatenate(\n",
    "        node_maxs.filter(regex='Peak Freqs$').loc[n, :]))\n",
    "    max_scaling = calc_max_scaling(w2_depth, w2_length, w2_width,\n",
    "                                   max_phys_depth, max_fp_length, max_fp_width)\n",
    "    scaled_hor_disp = scale_length(max_hor_disp, max_scaling)\n",
    "    scaled_vert_disp = scale_length(max_vert_disp, max_scaling)\n",
    "    scaled_mag_disp = scale_length(max_mag_disp, max_scaling)\n",
    "    scaled_seg_ten = scale_force(max_seg_ten, max_scaling)\n",
    "    scaled_peak_freqs = scale_freq(peak_freqs, max_scaling)\n",
    "\n",
    "    partition_nodes[n-1] = int(n)\n",
    "    partition_maxs['Max Disp Mag'][n-1] = max_mag_disp\n",
    "    partition_maxs['Max Hor Disp'][n-1] = max_hor_disp\n",
    "    partition_maxs['Max Vert Disp'][n-1] = max_vert_disp\n",
    "    partition_maxs['Peak Freqs'].append(peak_freqs)\n",
    "    partition_maxs['Max Tension'][n-1] = max_seg_ten\n",
    "    partition_maxs['Physical Depth'][n-1] = max_phys_depth\n",
    "    partition_maxs['Footprint Length'][n-1] = max_fp_length\n",
    "    partition_maxs['Footprint Width'][n-1] = max_fp_width\n",
    "    partition_maxs['Max Scaling'][n-1] = max_scaling\n",
    "    partition_maxs['Scaled Hor Disp'][n-1] = scaled_hor_disp\n",
    "    partition_maxs['Scaled Vert Disp'][n-1] = scaled_vert_disp\n",
    "    partition_maxs['Scaled Disp Mag'][n-1] = scaled_mag_disp\n",
    "    partition_maxs['Scaled Tension'][n-1] = scaled_seg_ten\n",
    "    partition_maxs['Scaled Peak Freqs'].append(scaled_peak_freqs)\n",
    "partition_maxs = pd.DataFrame(data=partition_maxs, index=partition_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_maxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_maxs.to_csv('results_VolturnUS-15MW_500m_semitaut.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale everything to 1/60th scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_60 = maxs = {\n",
    "    'Max Hor Disp': np.zeros(numnodes),\n",
    "    'Max Vert Disp': np.zeros(numnodes),\n",
    "    'Max Disp Mag': np.zeros(numnodes),\n",
    "    'Max Tension': np.zeros(numnodes),\n",
    "    'Peak Freqs': [],\n",
    "    'Physical Depth': np.zeros(numnodes),\n",
    "    'Footprint Length': np.zeros(numnodes),\n",
    "    'Footprint Width': np.zeros(numnodes),\n",
    "    'Max Scaling': np.zeros(numnodes),\n",
    "    'Scaled Hor Disp': np.zeros(numnodes),\n",
    "    'Scaled Vert Disp': np.zeros(numnodes),\n",
    "    'Scaled Disp Mag': np.zeros(numnodes),\n",
    "    'Scaled Tension': np.zeros(numnodes),\n",
    "    'Scaled Peak Freqs': []\n",
    "}\n",
    "partition_nodes = np.zeros(numnodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1, numnodes+1):\n",
    "    max_phys_depth = node_maxs.loc[numnodes-n:, 'Depth'].max()\n",
    "    max_fp_length = node_maxs.loc[numnodes-n:, 'Footprint Length'].max()\n",
    "    max_fp_width = node_maxs.loc[numnodes-n:, 'Footprint Width'].max()\n",
    "    max_hor_disp = node_maxs.filter(regex='Max Hor Disp$').loc[n, :].max()\n",
    "    max_vert_disp = node_maxs.filter(regex='Max Vert Disp$').loc[n, :].max()\n",
    "    max_mag_disp = node_maxs.filter(regex='Max Disp Mag$').loc[n, :].max()\n",
    "    max_seg_ten = node_maxs.filter(regex='Max Tension$').loc[n, :].max()\n",
    "    peak_freqs = np.unique(np.concatenate(\n",
    "        node_maxs.filter(regex='Peak Freqs$').loc[n, :]))\n",
    "    max_scaling = calc_max_scaling(w2_depth, w2_length, w2_width,\n",
    "                                   max_phys_depth, max_fp_length, max_fp_width)\n",
    "\n",
    "    hor_disp_60 = scale_length(max_hor_disp, scale)\n",
    "    vert_disp_60 = scale_length(max_vert_disp, scale)\n",
    "    mag_disp_60 = scale_length(max_mag_disp, scale)\n",
    "    seg_ten_60 = scale_force(max_seg_ten, scale)\n",
    "    peak_freqs_60 = scale_freq(peak_freqs, scale)\n",
    "\n",
    "    partition_nodes[n-1] = int(n)\n",
    "    partition_60['Max Disp Mag'][n-1] = max_mag_disp\n",
    "    partition_60['Max Hor Disp'][n-1] = max_hor_disp\n",
    "    partition_60['Max Vert Disp'][n-1] = max_vert_disp\n",
    "    partition_60['Peak Freqs'].append(peak_freqs)\n",
    "    partition_60['Max Tension'][n-1] = max_seg_ten\n",
    "    partition_60['Physical Depth'][n-1] = max_phys_depth\n",
    "    partition_60['Footprint Length'][n-1] = max_fp_length\n",
    "    partition_60['Footprint Width'][n-1] = max_fp_width\n",
    "    if max_scaling > scale:\n",
    "        partition_60['Scaled Hor Disp'][n-1] = np.nan\n",
    "        partition_60['Scaled Vert Disp'][n-1] = np.nan\n",
    "        partition_60['Scaled Disp Mag'][n-1] = np.nan\n",
    "        partition_60['Scaled Tension'][n-1] = np.nan\n",
    "        partition_60['Scaled Peak Freqs'].append(np.nan)\n",
    "    else:\n",
    "        partition_60['Scaled Hor Disp'][n-1] = hor_disp_60\n",
    "        partition_60['Scaled Vert Disp'][n-1] = vert_disp_60\n",
    "        partition_60['Scaled Disp Mag'][n-1] = mag_disp_60\n",
    "        partition_60['Scaled Tension'][n-1] = seg_ten_60\n",
    "        partition_60['Scaled Peak Freqs'].append(peak_freqs_60)\n",
    "partition_60 = pd.DataFrame(data=partition_60, index=partition_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wot_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
